{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spooky Author Identification - Identify authors from their writings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set size: 19579\n",
      "test set size: 8392\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('./input/train.csv')\n",
    "test = pd.read_csv('./input/test.csv')\n",
    "print(\"training set size: {}\".format(train.shape[0]))\n",
    "print(\"test set size: {}\".format(test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id26305</td>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id17569</td>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id11008</td>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id27763</td>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id12958</td>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text author\n",
       "0  id26305  This process, however, afforded me no means of...    EAP\n",
       "1  id17569  It never once occurred to me that the fumbling...    HPL\n",
       "2  id11008  In his left hand was a gold snuff box, from wh...    EAP\n",
       "3  id27763  How lovely is spring As we looked from Windsor...    MWS\n",
       "4  id12958  Finding nothing else, not even gold, the Super...    HPL"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a simple tokenizer function in order to remove stop words and punctuation characters. the Gensim algorithms work on sentences instead of arbitrary documents. This process is done automatically in `scikit-learn` vectorizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize(docs):\n",
    "    pattern = stopwords.words(\"english\") + list(punctuation)\n",
    "    sentences = []\n",
    "    for d in docs:\n",
    "        sentence = word_tokenize(d)\n",
    "        sentences.append([w for w in sentence if w not in pattern])\n",
    "    return sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = tokenize(train.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gnerate features and  Build models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models without word embeddings \n",
    "* multinomial NB \n",
    "* bernoulli NB \n",
    "* SVC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mutlinomial NB\n",
    "multi_nb = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(min_df=5, ngram_range=(1,3))),\n",
    "                    (\"mutlinomial nb\", MultinomialNB())])\n",
    "#bernoulli NB\n",
    "bernoulli_nb = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(min_df=5, ngram_range=(1,3))),\n",
    "                         (\"bernoulli nb\", BernoulliNB())])\n",
    "# SVM\n",
    "svc = Pipeline([(\"tfidf_vectorizer\", TfidfVectorizer(min_df=5, ngram_range=(1,3))),\n",
    "               (\"linear svc\", SVC(kernel=\"linear\", probability=True))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models with word embeddings \n",
    "* Extra Trees Classifier\n",
    "* Random Forest Classifier\n",
    "<br>\n",
    "\n",
    "The word2vec features will convert the documents from the sparse word-count features into only hundreds of dense features. \n",
    "<br>\n",
    "Following is an implementation of embedding vectorizer. With a given word, this vectorizes texts by taking the mean of all the vectors corresponding to individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "model = Word2Vec(sentences, size=300, window=5, min_count=5, workers=4)\n",
    "model.init_sims(replace=True) # unload memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.wv is KeyedVector which contain vectors and vocab for\n",
    "# the word2vec training class \n",
    "w2v = {w: vec for w,vec in zip(model.wv.index2word, model.wv.syn0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.word2weight = None\n",
    "        self.dim = 300\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        vect = TfidfVectorizer(min_df=5, ngram_range=(1,3))\n",
    "        vect.fit(X)\n",
    "        max_idf = max(vect.idf_)\n",
    "        self.word2weight = defaultdict(\n",
    "        lambda: max_idf, [(w, vect.idf_[i]) for w, i in vect.vocabulary_.items()]\n",
    "        )\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([\n",
    "                self.word2vec[w]*self.word2weight[w] \n",
    "                for w in words if w in self.word2vec] or \n",
    "                [np.zeros(self.dim)], axis=0) \n",
    "            for words in X\n",
    "        ])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etree = Pipeline([(\"word2vec vectorier\", EmbeddingVectorizer(w2v)),\n",
    "                  (\"extra trees\", ExtraTreesClassifier(n_estimators=100))\n",
    "])\n",
    "random_forest = Pipeline([(\"word2vec vectorier\", EmbeddingVectorizer(w2v)),\n",
    "                        (\"random forest\", RandomForestClassifier(n_estimators=100))   \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from tabulate import tabulate\n",
    "\n",
    "models = [\n",
    "    #(\"multi_nb\", multi_nb),\n",
    "    #(\"bernoulli_nb\", bernoulli_nb),\n",
    "    (\"svc\", svc),\n",
    "    #(\"etree\", etree),\n",
    "    #(\"random_forest\", random_forest)\n",
    "]\n",
    "\n",
    "scores =  [(name, cross_val_score(model, train.text, train.author, cv=5).mean())\n",
    "                for name, model in models]\n",
    "\n",
    "print(tabulate(scores, floatfmt=\".4f\", headers=(\"model\", \"score\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit pipeline(svc) on test.text(X_train) and test.author(y_train)\n",
    "svc.fit(train.text, train.author)\n",
    "\n",
    "# make predictions \n",
    "pred = svc.predict_proba(test.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'tfidf_vectorizer', 'linear svc', 'tfidf_vectorizer__analyzer', 'tfidf_vectorizer__binary', 'tfidf_vectorizer__decode_error', 'tfidf_vectorizer__dtype', 'tfidf_vectorizer__encoding', 'tfidf_vectorizer__input', 'tfidf_vectorizer__lowercase', 'tfidf_vectorizer__max_df', 'tfidf_vectorizer__max_features', 'tfidf_vectorizer__min_df', 'tfidf_vectorizer__ngram_range', 'tfidf_vectorizer__norm', 'tfidf_vectorizer__preprocessor', 'tfidf_vectorizer__smooth_idf', 'tfidf_vectorizer__stop_words', 'tfidf_vectorizer__strip_accents', 'tfidf_vectorizer__sublinear_tf', 'tfidf_vectorizer__token_pattern', 'tfidf_vectorizer__tokenizer', 'tfidf_vectorizer__use_idf', 'tfidf_vectorizer__vocabulary', 'linear svc__C', 'linear svc__cache_size', 'linear svc__class_weight', 'linear svc__coef0', 'linear svc__decision_function_shape', 'linear svc__degree', 'linear svc__gamma', 'linear svc__kernel', 'linear svc__max_iter', 'linear svc__probability', 'linear svc__random_state', 'linear svc__shrinking', 'linear svc__tol', 'linear svc__verbose'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkout parmeter names \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'linear svc__C': 1}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pg = {'linear svc__C':[0.1, 1, 10, 100]} # parameter_grid\n",
    "\n",
    "grid = GridSearchCV(svc, param_grid=pg, cv=5)\n",
    "\n",
    "grid.fit(train.text, train.author)\n",
    "\n",
    "print(grid.best_params_)\n",
    "\n",
    "# grid.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single word is represented as a vector of 300 numbers. In order to use the word2vec model to generate features for your ML algorithm,\n",
    "you need to convert your reviews into feature vectors. Represent a review document as the average vector of all words in the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_to_csv(pred, idx):\n",
    "    #file_title = str(pred_with_param.size)+\"_\"+str(pred_with_param.window)+\"_\"+str(pred_with_param.sample)+\"_\"+str(pred_min_count)+\"_\"+str(n_estimator)\n",
    "    results_df = pd.DataFrame(pred, columns=['EAP','HPL','MWS'])\n",
    "    merged_df = pd.concat([pd.Series(test.id), results_df], axis=1)\n",
    "    merged_df.to_csv(\"./output/output\"+str(idx)+\".csv\", sep=\",\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_to_csv_csv(pred, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(pred, columns=['EAP','HPL','MWS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_df = pd.concat([pd.Series(test.id), results_df], axis=1)\n",
    "merged_df.to_csv(\"./output/output3.csv\", sep=\",\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
